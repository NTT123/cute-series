{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN9oPquvU2ND7NAaV6IT0Qh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NTT123/cute-series/blob/main/day_00_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Hello! This is the first notebook in a series of notebooks in which I learn GPU programming with Cute DSL.\n",
        "\n",
        "[Cute DSL](https://docs.nvidia.com/cutlass/media/docs/pythonDSL/cute_dsl_general/dsl_introduction.html) is a domain-specific language (DSL) that allows us to do CUDA programming in Python directly. Cute (short for CUda TEnsor) provides a nice interface to manipulate data layouts, which is very important in GPU parallel programming. In this first notebook, we will explore how one can use Cute DSL to write very simple kernels."
      ],
      "metadata": {
        "id": "cqZPliK_vdYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0GID2eay3b6",
        "outputId": "8a377fbf-da79-46f1-f979-a5c914d6dd30"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-fbaa1d9a-bea0-63e9-852f-7ccab5a89d2a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Google Colab notebooks for this series. Google Colab provides free access to T4 GPUs; all you need is a Google account. As you can see with the `nvidia-smi -L` command, we have access to a Tesla T4 GPU.\n",
        "\n",
        "To use Cute DSL, we first need to install the `nvidia-cutlass-dsl` package. We use the ugly `sys.path.append` trick here to allow us to import `cutlass` without restarting the session.\n"
      ],
      "metadata": {
        "id": "ToMWJhPGy5zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip uninstall -y cuda-python cuda-bindings\n",
        "%pip install -Uq nvidia-cutlass-dsl==4.1.0 cuda-bindings==12.9.1 cuda-python==12.9.1 svgwrite\n",
        "import sys; sys.path.append(\"/usr/local/lib/python3.12/dist-packages/nvidia_cutlass_dsl/python_packages/\")"
      ],
      "metadata": {
        "id": "Z5LmjW-jt8Jo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca55650-a9c1-47d2-d7e9-5feec33c9d97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: cuda-python 12.9.1\n",
            "Uninstalling cuda-python-12.9.1:\n",
            "  Successfully uninstalled cuda-python-12.9.1\n",
            "Found existing installation: cuda-bindings 12.9.1\n",
            "Uninstalling cuda-bindings-12.9.1:\n",
            "  Successfully uninstalled cuda-bindings-12.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hello World Kernel\n",
        "\n",
        "Let's start with a hello world example:\n",
        "\n",
        "A simple GPU program that has 3 threads, each will get its ID and print out `Hello, world!` and its name.\n"
      ],
      "metadata": {
        "id": "pTvy1IU8EHmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hello_from_gpu.py\n",
        "import cutlass\n",
        "from cutlass import cute\n",
        "import torch\n",
        "\n",
        "\n",
        "@cute.kernel\n",
        "def hello_kernel():\n",
        "    tid, _, _ = cute.arch.thread_idx()\n",
        "    cute.printf(\"Hello, world! I am thread #%d.\", tid)\n",
        "\n",
        "\n",
        "@cute.jit\n",
        "def hello():\n",
        "    hello_kernel().launch(\n",
        "        grid=(1,1,1),\n",
        "        block=(3,1,1)\n",
        "    )\n",
        "\n",
        "cutlass.cuda.initialize_cuda_context()\n",
        "hello()\n",
        "torch.cuda.synchronize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X6Cr9nyzEQn",
        "outputId": "4234ef38-5d41-4a11-bb09-65aa8386f199"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hello_from_gpu.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python hello_from_gpu.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kxVG4deE7UE",
        "outputId": "e6c03708-f94e-4296-f1d0-0ab790f9f951"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, world! I am thread #0.\n",
            "Hello, world! I am thread #1.\n",
            "Hello, world! I am thread #2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `hello` function is decorated with `cute.jit`, which indicates this function will be compiled and run on the CPU. Meanwhile, the `hello_kernel` function is decorated with `cute.kernel`, which indicates this function will be compiled and run on the GPU.\n",
        "\n",
        "We use the function `cute.arch.thread_idx()` to get the `threadIdx` which is a integer tupple of size 3 for three dimension x, y, z of the block. Similarly, we have the function `cute.arch.block_idx()` to get blockIdx. To get the size of a block, one can use `cute.arch.block_dim()`.\n",
        "\n",
        "We use the syntax `hello_kernel().launch( grid=(1,1,1), block=(3,1,1) )` to launch the hello kernel, in which we specify a grid size of 1 block. And block size of 3 threads.\n",
        "\n",
        "The `cute.printf` will be run at runtime and can be very helpful for debugging. However, `cute.printf`'s output does not show up when we run it in a notebook cell ðŸ˜ž, so we will not use it often in this series.\n"
      ],
      "metadata": {
        "id": "RFyX9we6JOSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Vector Kernel\n",
        "Let's move on to a more practical example: adding two vectors.\n",
        "\n",
        "Given two vectors `A` and `B` of length `N`, compute the output vector `C = A + B`.\n"
      ],
      "metadata": {
        "id": "cWxpJM8kphmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.testing import assert_close\n",
        "\n",
        "import cutlass\n",
        "from cutlass import cute\n",
        "from cutlass.cute.runtime  import from_dlpack\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "@cute.kernel\n",
        "def add_vec_kernel(gA: cute.Tensor, gB: cute.Tensor, gC: cute.Tensor):\n",
        "    tid, _, _ = cute.arch.thread_idx()\n",
        "    bid, _, _ = cute.arch.block_idx()\n",
        "    blk_size, _, _ = cute.arch.block_dim()\n",
        "    i = tid + bid * blk_size\n",
        "\n",
        "    gC[i] = gA[i] + gB[i]\n",
        "\n",
        "\n",
        "@cute.jit\n",
        "def add_vec(gA: cute.Tensor, gB: cute.Tensor, gC: cute.Tensor):\n",
        "    N = cute.size(gA, [0])\n",
        "    add_vec_kernel(gA, gB, gC).launch(\n",
        "        grid = (N//1024, 1, 1),\n",
        "        block = (1024, 1, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "cutlass.cuda.initialize_cuda_context()\n",
        "torch.manual_seed(0)\n",
        "A = torch.randn(1024*512*1024, dtype=torch.float16, device='cuda')\n",
        "B = torch.randn_like(A)\n",
        "C = torch.empty_like(A)\n",
        "\n",
        "A_ = from_dlpack(A, assumed_align=256)\n",
        "B_ = from_dlpack(B, assumed_align=256)\n",
        "C_ = from_dlpack(C, assumed_align=256)\n",
        "\n",
        "add_vec(A_, B_, C_)\n",
        "assert_close(C, A+B)"
      ],
      "metadata": {
        "id": "raXF_W3NKKZK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We divide the vectors into blocks, each block taking care of 1024 elements in the vectors.\n",
        "For each block, we will use 1024 threads, each thread taking care of 1 element in the vectors.\n",
        "\n",
        "A GPU is a collection of Streaming Multiprocessors (SMs) that run mostly independently from each other (you can imagine an SM as a CPU core). By dividing the problem into independent blocks, we can simply send different blocks to different SMs and allow them to be processed in parallel.\n",
        "In our case, a T4 GPU has 40 SMs.\n",
        "\n",
        "Let's do a simple benchmarking to measure the runtime and memory bandwidth of our kernel.\n"
      ],
      "metadata": {
        "id": "dXy4wdwyRKry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "def benchmark(fn, *, num_bytes, warnup_iter, run_iter):\n",
        "    start_event = [torch.cuda.Event(enable_timing=True) for _ in range(run_iter)]\n",
        "    end_event = [torch.cuda.Event(enable_timing=True) for _ in range(run_iter)]\n",
        "    torch.cuda.synchronize()\n",
        "    for _ in range(warnup_iter):\n",
        "        fn()\n",
        "    for i in range(run_iter):\n",
        "        A.random_()\n",
        "        B.random_()\n",
        "        C.random_()\n",
        "        start_event[i].record()\n",
        "        fn()\n",
        "        end_event[i].record()\n",
        "    torch.cuda.synchronize()\n",
        "    ts = []\n",
        "    for i in range(run_iter):\n",
        "        elapsed_time = start_event[i].elapsed_time(end_event[i])\n",
        "        ts.append(elapsed_time)\n",
        "    avg_time = sum(ts) / len(ts)\n",
        "    mem_bandwidth = num_bytes / (1e9) / (avg_time / 1e3)\n",
        "    print(f\"Avg time: {avg_time:.2f} ms\")\n",
        "    print(f\"Bandwidth: {mem_bandwidth:.2f} GB/s\")\n",
        "\n",
        "\n",
        "# total read + write bytes\n",
        "NUM_BYTES = A.nbytes * 3\n",
        "compiled_kernel = cute.compile(add_vec, A_, B_, C_)\n",
        "benchmark(lambda: compiled_kernel(A_, B_, C_), num_bytes=NUM_BYTES, warnup_iter=100, run_iter=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WMXAyFzSF0P",
        "outputId": "d7728176-d0a7-4267-891f-1b7081d30072"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg time: 15.72 ms\n",
            "Bandwidth: 204.93 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the `add_vec_kernel` kernel, we can divide it into two sub-tasks:\n",
        "\n",
        "* **data layout**: the first four lines compute the index of the item that the current thread is working on,\n",
        "* **computation**: the last line does the actual computation.\n",
        "\n",
        "It is a common task in GPU programming to specify how the data is laid out on the GPU hardware. Cute is designed to help us with this task!\n",
        "\n",
        "A layout is a linear map from a set of integer coordinates to a flat coordinate. A layout is defined by its shape and stride for each coordinate. The output coordinate is computed as the dot product between the input coordinates and the stride.\n",
        "\n",
        "```python\n",
        "layout = cute.make_layout(shape=(4, 8), stride=(1, 4))\n",
        "```\n",
        "\n",
        "We can use the `cute.make_layout` function to create a layout. The layout in the above example has a 2D shape of `(4, 8)` and a stride of `(1, 4)`. So, a coordinate of `(2, 5)` will be mapped to `2 x 1 + 5 x 4 = 22`.\n",
        "\n",
        "From this, we can build a data layout for our `add-vector` kernel as follows.\n"
      ],
      "metadata": {
        "id": "UGN0BYsYSJDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cute.kernel\n",
        "def add_vec_kernel_w_layout(\n",
        "    gA: cute.Tensor,\n",
        "    gB: cute.Tensor,\n",
        "    gC: cute.Tensor,\n",
        "    layout: cute.Layout\n",
        "):\n",
        "    tid, _, _ = cute.arch.thread_idx()\n",
        "    bid, _, _ = cute.arch.block_idx()\n",
        "    thrA = cute.composition(gA, layout)[None, tid, bid]\n",
        "    thrB = cute.composition(gB, layout)[None, tid, bid]\n",
        "    thrC = cute.composition(gC, layout)[None, tid, bid]\n",
        "    thrC[None] = thrA.load() + thrB.load()\n",
        "\n",
        "\n",
        "@cute.jit\n",
        "def add_vec_layout(mA: cute.Tensor, mB: cute.Tensor, mC: cute.Tensor):\n",
        "    N = cute.size(mA, [0])\n",
        "    # value-thread-block layout\n",
        "    layout = cute.make_layout(\n",
        "        shape=(1, 1024, N//1024),\n",
        "    )\n",
        "\n",
        "    grid_size = cute.size(layout, [2])\n",
        "    block_size = cute.size(layout, [1])\n",
        "\n",
        "    add_vec_kernel_w_layout(mA, mB, mC, layout).launch(\n",
        "        grid = (grid_size, 1, 1),\n",
        "        block = (block_size, 1, 1)\n",
        "    )\n",
        "\n",
        "C.zero_()\n",
        "add_vec_layout(A_, B_, C_)\n",
        "assert_close(C, A+B)\n",
        "\n",
        "compiled_kernel = cute.compile(add_vec_layout, A_, B_, C_)\n",
        "benchmark(lambda: compiled_kernel(A_, B_, C_), num_bytes=NUM_BYTES, warnup_iter=20, run_iter=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rORyCbcDdpTl",
        "outputId": "ffecae8c-a21e-47e9-a87c-33d2c12b18ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg time: 15.71 ms\n",
            "Bandwidth: 205.07 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a value-thread-block layout for our data that allows us to locate the data using `block_idx` and `thread_idx`. Note that when the stride is not specified, it is computed automatically such that the first coordinate has stride 1. This is different from `torch.Tensor`â€™s default stride, where the last coordinate has stride 1.\n",
        "\n",
        "A `cute.Tensor` is simply a pair of a data source and a layout. This allows us to compute the composition of a tensor and a layout. For example, `cute.composition(gA, layout)[None, None, bid]` will return a new tensor that has a layout mapping `(value_idx, thread_idx, block_idx)` to the physical address (actually, the offset) of a data item in `gA`.\n",
        "\n",
        "In summary, `cute.composition(gA, layout)[None, tid, bid]` return a slice of `gA` that belongs to the current thread."
      ],
      "metadata": {
        "id": "yKih0mqkrytC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faster Add Vector Kernel\n",
        "\n",
        "In the above kernel, each thread loads 1 data item of size 2 bytes (float16) from global memory to a register. A warp of 32 threads will load a total of 64 contiguous bytes from global memory. However, this does not saturate the memory bandwidth, as the T4 GPU can load a total of 128 bytes at once. To fully saturate the memory bandwidth, each thread should load 2 items (4 bytes) at once. We can easily do that by modifying our layout. The rest of the kernel needs no change!\n"
      ],
      "metadata": {
        "id": "zme1WWDKQSLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cute.kernel\n",
        "def faster_add_vec_kernel_w_layout(\n",
        "    gA: cute.Tensor,\n",
        "    gB: cute.Tensor,\n",
        "    gC: cute.Tensor,\n",
        "    btv_layout: cute.Layout\n",
        "):\n",
        "    tid, _, _ = cute.arch.thread_idx()\n",
        "    bid, _, _ = cute.arch.block_idx()\n",
        "    thrA = cute.composition(gA, btv_layout)[None, tid, bid]\n",
        "    thrB = cute.composition(gB, btv_layout)[None, tid, bid]\n",
        "    thrC = cute.composition(gC, btv_layout)[None, tid, bid]\n",
        "    thrC[None] = thrA.load() + thrB.load()\n",
        "\n",
        "\n",
        "@cute.jit\n",
        "def faster_add_vec_layout(mA: cute.Tensor, mB: cute.Tensor, mC: cute.Tensor):\n",
        "    N = cute.size(mA, [0])\n",
        "    # value-thread-block layout\n",
        "    btv_layout = cute.make_layout(\n",
        "        shape=(2, 512, N//1024),\n",
        "    )\n",
        "\n",
        "    grid_size = cute.size(btv_layout, [2])\n",
        "    block_size = cute.size(btv_layout, [1])\n",
        "\n",
        "    faster_add_vec_kernel_w_layout(mA, mB, mC, btv_layout).launch(\n",
        "        grid = (grid_size, 1, 1),\n",
        "        block = (block_size, 1, 1)\n",
        "    )\n",
        "\n",
        "C.zero_()\n",
        "add_vec_layout(A_, B_, C_)\n",
        "assert_close(C, A+B)\n",
        "\n",
        "compiled_kernel = cute.compile(faster_add_vec_layout, A_, B_, C_)\n",
        "benchmark(lambda: compiled_kernel(A_, B_, C_), num_bytes=NUM_BYTES, warnup_iter=20, run_iter=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIcIjAvOMa5m",
        "outputId": "01a61c5f-7838-4638-cb9a-c759b5958037"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg time: 12.22 ms\n",
            "Bandwidth: 263.69 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profiling\n",
        "\n",
        "Finally, let's profile the kernel with `ncu` to check if our kernel is in good shape.\n"
      ],
      "metadata": {
        "id": "Xd1mgdr-1Ytj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add_vec_kernel.py\n",
        "import torch\n",
        "from torch.testing import assert_close\n",
        "\n",
        "import cutlass\n",
        "from cutlass import cute\n",
        "from cutlass.cute.runtime  import from_dlpack\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "@cute.kernel\n",
        "def add_vec_kernel(gA: cute.Tensor, gB: cute.Tensor, gC: cute.Tensor):\n",
        "    tid, _, _ = cute.arch.thread_idx()\n",
        "    bid, _, _ = cute.arch.block_idx()\n",
        "    blk_size, _, _ = cute.arch.block_dim()\n",
        "    i = tid + bid * blk_size\n",
        "\n",
        "    gC[i] = gA[i] + gB[i]\n",
        "\n",
        "\n",
        "@cute.jit\n",
        "def add_vec(gA: cute.Tensor, gB: cute.Tensor, gC: cute.Tensor):\n",
        "    N = cute.size(gA, [0])\n",
        "    add_vec_kernel(gA, gB, gC).launch(\n",
        "        grid = (N//1024, 1, 1),\n",
        "        block = (1024, 1, 1)\n",
        "    )\n",
        "\n",
        "\n",
        "cutlass.cuda.initialize_cuda_context()\n",
        "torch.manual_seed(0)\n",
        "A = torch.randn(1024*512*1024, dtype=torch.float16, device='cuda')\n",
        "B = torch.randn_like(A)\n",
        "C = torch.empty_like(A)\n",
        "\n",
        "A_ = from_dlpack(A, assumed_align=256)\n",
        "B_ = from_dlpack(B, assumed_align=256)\n",
        "C_ = from_dlpack(C, assumed_align=256)\n",
        "\n",
        "add_vec(A_, B_, C_)\n",
        "assert_close(C, A+B)\n",
        "\n",
        "\n",
        "@cute.kernel\n",
        "def faster_add_vec_kernel_w_layout(\n",
        "    gA: cute.Tensor,\n",
        "    gB: cute.Tensor,\n",
        "    gC: cute.Tensor,\n",
        "    btv_layout: cute.Layout\n",
        "):\n",
        "    tid, _, _ = cute.arch.thread_idx()\n",
        "    bid, _, _ = cute.arch.block_idx()\n",
        "    thrA = cute.composition(gA, btv_layout)[None, tid, bid]\n",
        "    thrB = cute.composition(gB, btv_layout)[None, tid, bid]\n",
        "    thrC = cute.composition(gC, btv_layout)[None, tid, bid]\n",
        "    thrC[None] = thrA.load() + thrB.load()\n",
        "\n",
        "\n",
        "@cute.jit\n",
        "def faster_add_vec_layout(mA: cute.Tensor, mB: cute.Tensor, mC: cute.Tensor):\n",
        "    N = cute.size(mA, [0])\n",
        "    # value-thread-block layout\n",
        "    btv_layout = cute.make_layout(\n",
        "        shape=(2, 512, N//1024),\n",
        "    )\n",
        "\n",
        "    grid_size = cute.size(btv_layout, [2])\n",
        "    block_size = cute.size(btv_layout, [1])\n",
        "\n",
        "    faster_add_vec_kernel_w_layout(mA, mB, mC, btv_layout).launch(\n",
        "        grid = (grid_size, 1, 1),\n",
        "        block = (block_size, 1, 1)\n",
        "    )\n",
        "\n",
        "C.zero_()\n",
        "faster_add_vec_layout(A_, B_, C_)\n",
        "assert_close(C, A+B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obaroyv70gRV",
        "outputId": "7875662f-23c5-43d1-b95c-0aa215c70740"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting add_vec_kernel.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del A, B, C, A_, B_, C_\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Taq2IlAR1p5l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu -k 'regex:kernel_cutlass_.+' python add_vec_kernel.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U91uFaOp0zMV",
        "outputId": "3143132e-9914-4984-e820-08bf6bd23c0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 4512 (/usr/bin/python3.12)\n",
            "==PROF== Profiling \"kernel_cutlass_add_vec_kernel_tensorptrf16gmemalign256o5368709121_tensorptrf16gmemalign256o5368709121_tensorptrf16gmemalign256o5368709121_0\": 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"kernel_cutlass_faster_add_vec_kernel_w_layout_tensorptrf16gmemalign256o5368709121_tensorptrf16gmemalign256o5368709121_tensorptrf16gmemalign256o5368709121_0\": 0%\n",
            "==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using \"--replay-mode application\" to avoid memory save-and-restore.\n",
            "....50%....100% - 9 passes\n",
            "==PROF== Disconnected from process 4512\n",
            "[4512] python3.12@127.0.0.1\n",
            "  kernel_cutlass_add_vec_kernel_tensorptrf16gmemalign256o5368709121_tensorptrf16gmemalign256o5368709121_tensorptrf16gmemalign256o5368709121_0 (524288, 1, 1)x(1024, 1, 1), Context 2, Stream 14, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle   12,032,015\n",
            "    Memory Throughput                 %        59.84\n",
            "    DRAM Throughput                   %        59.84\n",
            "    Duration                         ms        20.57\n",
            "    L1/TEX Cache Throughput           %        33.67\n",
            "    L2 Cache Throughput               %        17.89\n",
            "    SM Active Cycles              cycle 9,965,724.57\n",
            "    Compute (SM) Throughput           %        27.89\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n",
            "          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n",
            "          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                 1,024\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     536,870,912\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                           13,107.20\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            4\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        84.88\n",
            "    Achieved Active Warps Per SM           warp        27.16\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 15.12%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (84.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle    61,508,508\n",
            "    Total DRAM Elapsed Cycles        cycle   822,238,208\n",
            "    Average L1 Active Cycles         cycle  9,965,724.57\n",
            "    Total L1 Elapsed Cycles          cycle   481,279,760\n",
            "    Average L2 Active Cycles         cycle    17,404,630\n",
            "    Total L2 Elapsed Cycles          cycle   562,727,904\n",
            "    Average SM Active Cycles         cycle  9,965,724.57\n",
            "    Total SM Elapsed Cycles          cycle   481,279,760\n",
            "    Average SMSP Active Cycles       cycle  9,575,011.91\n",
            "    Total SMSP Elapsed Cycles        cycle 1,925,119,040\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  kernel_cutlass_faster_add_vec_kernel_w_layout_tensorptrf16gmemalign256o5368709121_tensorptrf16gmemalign256o5368709121_tensorptrf16gmemalign256o5368709121_0 (524288, 1, 1)x(512, 1, 1), Context 2, Stream 14, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.98\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    7,206,206\n",
            "    Memory Throughput                 %        90.28\n",
            "    DRAM Throughput                   %        90.28\n",
            "    Duration                         ms        12.32\n",
            "    L1/TEX Cache Throughput           %        30.05\n",
            "    L2 Cache Throughput               %        29.87\n",
            "    SM Active Cycles              cycle 6,936,361.83\n",
            "    Compute (SM) Throughput           %        23.35\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   512\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     268,435,456\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            6,553.60\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            8\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            2\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.28\n",
            "    Achieved Active Warps Per SM           warp        26.33\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.72%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.3%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 55,414,748.50\n",
            "    Total DRAM Elapsed Cycles        cycle   491,028,480\n",
            "    Average L1 Active Cycles         cycle  6,936,361.83\n",
            "    Total L1 Elapsed Cycles          cycle   287,459,648\n",
            "    Average L2 Active Cycles         cycle 10,501,187.91\n",
            "    Total L2 Elapsed Cycles          cycle   337,028,288\n",
            "    Average SM Active Cycles         cycle  6,936,361.83\n",
            "    Total SM Elapsed Cycles          cycle   287,459,648\n",
            "    Average SMSP Active Cycles       cycle  6,848,472.41\n",
            "    Total SMSP Elapsed Cycles        cycle 1,149,838,592\n",
            "    -------------------------- ----------- -------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the original kernel (`kernel_cutlass_add_vec_kernel`) only achieved a peak memory throughput of 60%, while our best kernel (`kernel_cutlass_faster_add_vec_kernel_w_layout`) achieved a peak memory throughput of 90% ðŸ˜€.\n",
        "\n",
        "---\n",
        "\n",
        "THE END\n"
      ],
      "metadata": {
        "id": "P2kH7y6x2nxO"
      }
    }
  ]
}